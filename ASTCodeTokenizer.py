from tree_sitter import Parser
import json
import string
import random
from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors, decoders
class ASTCodeTokenizer:
    
    def __init__(self, parser=None):
        
        # Define special tokens
        self.special_tokens = {
            '__unk__':0,
            '__END_OF_CODE__': 1,
            '__PAD__': 2,
            '__MASKVAR__': 3
        }
        # Add 200 distinct MASK tokens
        self.masked_token_range = 200
        for i in range(self.masked_token_range):
            self.special_tokens[f'__MASK{i}__'] = len(self.special_tokens)
        self.vocab = {**self.special_tokens}
        self.inverse_vocab = {v: k for k, v in self.vocab.items()}
        self.token_id_counter = len(self.vocab)
        self.parser = parser
        
        self.tokens = []
        
    def save_vocab(self,file_path, vocab):
        with open(file_path, 'w') as json_file: 
            json.dump(vocab, json_file, ensure_ascii=False, indent=4)

    def train_bpe_tokenizer(self, file_list, vocab_size=50000, min_frequency=2):
        # Initialize a BPE tokenizer model
        self.bpe_tokenizer = Tokenizer(models.BPE(unk_token="__unk__"))

        # Set up pre-tokenizer (you can adjust this as needed)
        self.bpe_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()

        # Initialize a BPE trainer with a vocabulary size limit
        trainer = trainers.BpeTrainer(vocab_size=vocab_size, min_frequency=min_frequency)

        # Train tokenizer on the provided list of files
        self.bpe_tokenizer.train(files=file_list, trainer=trainer)

        # Set up the post-processor to handle special tokens
        self.bpe_tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)

        # Define the decoder (to convert token IDs back into readable text)
        self.bpe_tokenizer.decoder = decoders.ByteLevel()

        self.save_vocab('bpe_vocab.json', self.bpe_tokenizer.get_vocab())
        
    def extract_edges_from_ast(self, code):
        """
        Extract edges from an AST generated by tree-sitter.
        Args:
            ast: The root node of the AST.
        Returns:
            edges: List of tuples (source_index, target_index, edge_type).
        """
        code_bytes = code.encode()
        edges = []
        tree = self.parser.parse(code_bytes)
        root_node = tree.root_node
        def traverse(node, parent_index=None):
            current_index = node.start_point[0]  # Use start line as the node index
            if parent_index is not None:
                edges.append((parent_index, current_index, "parent-child"))
                edges.append((current_index, parent_index, "child-parent"))

            for child in node.children:
                traverse(child, current_index)

        traverse(root_node)
        return edges

    def anonymize_variable_names_for_tla(self, code):
        placeholder_pool = ["var", "state", "element"]
        variable_map = {}  # Map original variable names to placeholders
        code_bytes = code.encode()
        anonymized_code = code_bytes.decode('utf-8')
        replacements = []  # List to collect (start_byte, end_byte, replacement) tuples
        tree = self.parser.parse(code_bytes)
        root_node = tree.root_node
        def traverse_and_collect_variables(node):
            """Traverse the AST and collect variable declarations."""
            if node.type in  {'variable_declaration'}:
                for child in node.children:
                    if child.type == 'identifier':  # Found a variable name
                        var_name = code_bytes[child.start_byte:child.end_byte].decode('utf-8')
                        if var_name not in variable_map:
                            characters = string.ascii_letters
                            placeholder =  random.choice(placeholder_pool) + str(len(variable_map)) #''.join(random.choice(characters) for i in range(3))
                            variable_map[var_name] = placeholder
    
            # Recursively process child nodes
            for child in node.children:
                traverse_and_collect_variables(child)
    
        def traverse_and_replace(node):
            """Traverse the AST and replace variables in the code."""
            if node.type in {'identifier_ref','identifier'} :
                var_name = code_bytes[node.start_byte:node.end_byte].decode('utf-8')
                if var_name in variable_map:
                    # Replace the variable name in the code
                    placeholder = variable_map[var_name]
                    replacements.append((node.start_byte, node.end_byte, placeholder))
            # Recursively process child nodes
            for child in node.children:
                traverse_and_replace(child)

        def apply_replacements():
            """Apply collected replacements to the code string."""
            nonlocal anonymized_code
            for start_byte, end_byte, replacement in sorted(replacements, reverse=True):
                anonymized_code = (
                    anonymized_code[:start_byte] +
                    replacement +
                    anonymized_code[end_byte:]
                )
        
        # Step 1: Collect variables
        traverse_and_collect_variables(root_node)
        # Step 2: Replace variables in the code
        traverse_and_replace(root_node)
        # Step 3: Apply replacements
        apply_replacements()

        return anonymized_code
      
    def __parse_code(self, code):
        
        code_bytes = code.encode()
        tokens = []
        tree = self.parser.parse(code_bytes)
        root_node = tree.root_node
        ignored_nodes = {'block_comment', 'line_comment', 'comment', 'block_comment', 'block_comment_text', 'extramodular_text',
                                 'import_declaration', 'package_declaration','annotation','pcal_algorithm'}
        def traverse_node(node):
            if node.type not in ignored_nodes:
                # Check if the node is a method invocation and corresponds to System.out.print
                if node.type == 'method_invocation':
                    # Get the text of the receiver (e.g., "System.out")
                    receiver = None
                    for child in node.children:
                        if child.type == 'field_access':  # Field access like "System.out"
                            receiver = code_bytes[child.start_byte:child.end_byte].decode('utf-8')
                            break
                    # Skip the node if it matches "System.out.print"
                    if receiver and receiver.startswith("System.out"):
                        return
                """Traverse the AST and collect terminal tokens."""
                if node.child_count == 0:
                    # Extract the actual code text corresponding to this terminal node
                    token = code_bytes[node.start_byte:node.end_byte].decode('utf-8')
                    tokens.append(token)
                else:
                    for child in node.children:
                        traverse_node(child)
        traverse_node(root_node)

        return tokens
        

    def extract_all_java_class_fields(self, code):
        code_bytes = code.encode()
        tree = self.parser.parse(code_bytes)
        root_node = tree.root_node
        field_names = []
        def traverse_node(node):
            if node.type in {'field_declaration'}:
                for child in node.children: 
                    if child.type == 'variable_declarator':
                        # Find the identifier node within the variable_declarator
                        for grandchild in child.children:
                            if grandchild.type == 'identifier':  # Adjust this based on the grammar
                                token = code_bytes[grandchild.start_byte:grandchild.end_byte].decode('utf-8')
                                field_names.append(token)
                    
            else:
                for child in node.children:
                    traverse_node(child)
        traverse_node(root_node)

        return field_names
    
    def extract_all_tla_state_variables(self, spec):
        code_bytes = spec.encode()
        tree = self.parser.parse(code_bytes)
        root_node = tree.root_node
        state_variables = []
        def traverse_node(node):
            if node.type in {'variable_declaration'}:
                for child in node.children: 
                    if child.type == 'identifier':
                        token = code_bytes[child.start_byte:child.end_byte].decode('utf-8')
                        state_variables.append(token)
                    
            else:
                for child in node.children:
                    traverse_node(child)
        traverse_node(root_node)

        return state_variables

    def extract_all_keywords(self, code):
        
        all_tla_keywords = ["ASSUME", "ELSE", "LOCAL", "UNION", "ASSUMPTION", "ENABLED", "MODULE", "VARIABLE", "AXIOM", "EXCEPT", 
                    "OTHER", "VARIABLES", "CASE", "EXTENDS", "SF_", "WF_", "CHOOSE", "IF", "SUBSET", "WITH", "CONSTANT", "IN", "THEN", 
                    "CONSTANTS", "INSTANCE", "THEOREM", "COROLLARY", "DOMAIN", "LET", "UNCHANGED", "BY", "HAVE", "QED", "TAKE", "DEF", "HIDE", 
                    "RECURSIVE", "USE", "DEFINE", "PROOF", "WITNESS", "PICK", "DEFS", "PROVE", "SUFFICES", "NEW", "LAMBDA", "STATE", "ACTION", 
                    "TEMPORAL", "OBVIOUS", "OMITTED", "LEMMA", "PROPOSITION", "ONLY"]

        all_java_keywords = [
        "abstract", "assert", "boolean", "break", "byte", "case", "catch", 
        "char", "class", "const", "continue", "default", "do", "double", 
        "else", "enum", "extends", "final", "finally", "float", "for", 
        "goto", "if", "implements", "import", "instanceof", "int", 
        "interface", "long", "native", "new", "null", "package", "private", 
        "protected", "public", "return", "short", "static", "strictfp", 
        "super", "switch", "synchronized", "this", "throw", "throws", 
        "transient", "try", "void", "volatile", "while"]

        code_bytes = code.encode()
        tree = self.parser.parse(code_bytes)
        root_node = tree.root_node
        seen_keywords = set()
        def traverse_node(node):
            if node.type not in {'block_comment', 'line_comment', 'comment', 'block_comment', 'block_comment_text', 'extramodular_text',
                                 'import_declaration', 'package_declaration','annotation'}:
                if node.child_count == 0:
                    token = code_bytes[node.start_byte:node.end_byte].decode('utf-8')
                    if token in (all_tla_keywords + all_java_keywords):
                        seen_keywords.add(token)
                        
                else:
                    for child in node.children:
                        traverse_node(child)
        traverse_node(root_node)
        
        return seen_keywords

    def encode(self, code):
        self.tokens = []  # Reset tokens for each new encode call
        self.token_types = []  # Reset token types for each new encode call
        token_ids = []  # Final list of token IDs (custom tokens and subwords)
        self.tokens = self.__parse_code(code)
        """Add tokens to the vocabulary if not already present."""
        for token in self.tokens:
            if token in self.vocab:
                # Directly use the token ID from the vocab if it's present
                token_ids.append(self.vocab[token])
            
            # Fallback to BPE if vocab len is more than max_vocab_len 

            else:
                max_vocab_len = 10000
                if len(self.vocab) < max_vocab_len:
                    self.vocab[token] = self.token_id_counter     
                    self.inverse_vocab[self.token_id_counter] = token
                    self.token_id_counter += 1
                    token_ids.append(self.vocab[token])
                else:
                    encoded = self.bpe_tokenizer.encode(token)
                    for subword_id in encoded.ids:
                        # Convert BPE subword ID to subword token
                        subtoken = self.bpe_tokenizer.id_to_token(subword_id)
                        # Add subword token to vocab if not present
                        if subtoken not in self.vocab:
                            self.vocab[subtoken] = self.token_id_counter     
                            self.inverse_vocab[self.token_id_counter] = subtoken
                            self.token_id_counter += 1
                        # Append the subword token ID (either existing or newly added) to the final list
                        token_ids.append(self.vocab[subtoken])
        
       
        return token_ids
    
    def get_vocab_size(self):
        return len(self.vocab)
    
    # To make the parser object non-pickleable
    def __getstate__(self):
        state = self.__dict__.copy()
        state['parser'] = None  # Exclude parser from being pickled
        return state

    # After loading, we reinitialize the parser
    def __setstate__(self, state):
        self.__dict__.update(state)
        self.parser = Parser()  # Reinitialize the parser after loading
